"""
TODO
"""
from collections import namedtuple
import inspect
from multiprocessing import cpu_count
from multiprocessing.queues import Queue
from multiprocessing.process import Process
import os
import threading
import uuid

from orges.args import call


def determine_package(function):
    """
    Resolves a call by function to a call by package.
    - Determine absolute package name of the given function.
    - When the task gets executed, the worker process will import it.
    """

    # expand the module's path to an absolute import
    filename = inspect.getsourcefile(function)
    module_path, module_filename = os.path.split(filename)
    module_name, module_ext = os.path.splitext(module_filename)
    prefix = []
    for directory in module_path.split(os.sep)[::-1]:
        prefix.append(directory)
        candidate = ".".join(prefix[::-1] + [module_name])
        #print(candidate)
        try:
            __import__(candidate, globals(), locals(), [], 0)
            f_package = candidate
            return f_package
        except ImportError:
            pass
    raise ImportError("Could not determine the package of the given " +
                      "function. This should not happen.")


class _Singleton(type):
    """Thread-safe singleton."""

    _instances = {}
    _instances_lock = threading.Lock()

    def __call__(cls, *args, **kwargs):
        try:
            return cls._instances[cls]
        except KeyError:
            with cls._instances_lock:
                cls._instances[cls] = super(_Singleton, cls).__call__(*args, **kwargs)
            return cls._instances[cls]


class Singleton(_Singleton('SingletonMeta', (object,), {})):
    pass


class WorkerProvider(Singleton):
    """
    Keeps track of as many worker processes as there are CPUs.
    """

    def __init__(self):
        self._lock = threading.Lock()
        self._cpu_count = cpu_count()
        self._workers = []

    def provision(self, number_of_workers=1, queue_tasks=None,
                  queue_results=None, queue_status=None):
        """
        Provision a given number worker processes for future use.
        """
        with self._lock:
            if self._cpu_count < (len(self._workers) + number_of_workers):
                raise IndexError("Cannot currently provision so many workers.")

            workers = []
            for _ in range(number_of_workers):
                worker_id = uuid.uuid4()
                worker_process = WorkerProcess(worker_id=worker_id,
                                 queue_tasks=queue_tasks,
                                 queue_results=queue_results,
                                 queue_status=queue_status)
                worker_process.daemon = True  # workers may not spawn processes
                worker_process.start()
                workers.append(worker_process)

            self._workers += workers
        return workers

    def release(self, worker):
        with self._lock:
            self._workers.remove(worker)


class TaskHandle(object):
    def __init__(self, invoker, worker_id, task_id):
        self._invoker = invoker
        self._worker_id = worker_id
        self.task_id = task_id

    def cancel(self):
        self._invoker.cancel(self._worker_id, self.task_id)


# data structure for results generated by the workers
Task = namedtuple("Task", ["task_id", "f_package", "args", "vargs"])

# data structure for results generated by the workers
Status = namedtuple("Status", ["task_id", "f_package", "args", "vargs",
                               "worker_id"])

# data structure for results generated by the workers
Result = namedtuple("Result", ["task_id", "f_package", "args", "vargs",
                               "worker_id", "value"])


class WorkerProcess(Process):
    """Calls functions with arguments, both given by a queue."""
    def __init__(self, worker_id, queue_tasks, queue_results, queue_status):
        self._worker_id = worker_id
        self._queue_tasks = queue_tasks
        self._queue_results = queue_results
        self._queue_status = queue_status
        self._busy = False
        self._current_task_id = None
        super(WorkerProcess, self).__init__()

    @property
    def worker_id(self):
        """Property for the worker_id attribute of this class."""
        return self._worker_id

    @property
    def queue_tasks(self):
        """Property for the tasks attribute of this class."""
        return self._queue_tasks

    @property
    def queue_status(self):
        """Property for the results attribute of this class."""
        return self._queue_status

    @property
    def queue_results(self):
        """Property for the results attribute of this class."""
        return self._queue_results

    @property
    def busy(self):
        """Property for the results attribute of this class."""
        return self._busy

    @property
    def current_task_id(self):
        """Property for the results attribute of this class."""
        return self._current_task_id

    def run(self):
        for task in iter(self.queue_tasks.get, None):
            self._busy = True
            self._current_task_id = task.task_id
            # announce start of work
            #multiprocessing.log_to_stderr(logging.WARN).warning("a")
            status = Status(task_id=self._current_task_id,
                             worker_id=self._worker_id,
                            f_package=task.f_package,
                            args=task.args, vargs=task.vargs)
            self._queue_status.put(status)

            # import f given by qualified package name
            f = __import__(task.f_package, globals(), locals(), ['f'], 0).f
            # Note that the following is equivalent:
            #from orges.test.integration.invoker.multiprocess import f as f

            # the actual call
            value = call(f, task.args)

            #multiprocessing.log_to_stderr(logging.WARN).warning("b")

            # report result back
            self._queue_results.put(Result(task_id=self._current_task_id,
                                           worker_id=self._worker_id,
                                           f_package=task.f_package,
                                           args=task.args, value=value,
                                           vargs=task.vargs))
            #multiprocessing.log_to_stderr(logging.WARN).warning("c")
            self._busy = False
        # send sentinel back
        #multiprocessing.log_to_stderr(logging.WARN).warning("d")
        self._queue_results.put(Result(id=self._worker_id, f=None,
                                          args=None, vargs=None, value=None))
        #multiprocessing.log_to_stderr(logging.WARN).warning("e")
